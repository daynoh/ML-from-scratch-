{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "00112931",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8180c",
   "metadata": {},
   "source": [
    "### Processing timeSeries using reccurent neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bd9fa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    \n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10)) #\n",
    "    #wave 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # +\n",
    "    #wave 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)\n",
    "    # +noise\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89aa275",
   "metadata": {},
   "source": [
    "### Creating a training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49ccae07",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "\n",
    "x_train, y_train = series[:70000:n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8862d982",
   "metadata": {},
   "source": [
    "### Getting a baseline metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4bdd203",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = X_valid[:, -1]\n",
    "\n",
    "np.mean(keras.losses.mean_squared_error(y_valid, y_pred))\n",
    "\n",
    "\n",
    "### using the previous time step as out put calculate mse\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape = [50,1]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6738093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7481007"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TO DO COMPILE MODEL AND THEN FIT IT\n",
    "np.max(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8166783",
   "metadata": {},
   "source": [
    "### Creating a simPLE dEEP RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28d6331",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences = True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences = True), \n",
    "    keras.layers.SimpleRNN(1)# if you only care about last output  only\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742ac42f",
   "metadata": {},
   "source": [
    "### For faster implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f53f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences = True, input_shape = [None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences = True), \n",
    "    keras.layers.Dense(1)# if you only care about last output  only\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a005f74",
   "metadata": {},
   "source": [
    "### Predicting several time Steps ahead\n",
    "\n",
    "> Method 1: predicting on time step ahead and then appending that prediction to data used and then predictin it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(1, n_steps + 10)\n",
    "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "Y_pred = X[:, n_steps:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df19fac9",
   "metadata": {},
   "source": [
    "> Method 2: training an RNN to predict all 10 next values at once\n",
    "    \n",
    "    To use a sequence to vector model we will have to change how we create the the\n",
    "    training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b155b389",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_series(10000, n_steps + 10)\n",
    "X_train, Y_train = series[:7000, :n_steps], series[:7000, -10:, 0]\n",
    "X_valid, Y_valid = series[7000:9000, :n_steps], series[7000:9000, -10:,\n",
    "0]\n",
    "X_test, Y_test = series[9000:, :n_steps], series[9000:, -10:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4552c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The resulting network will look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2316d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,\n",
    "1]),\n",
    "keras.layers.SimpleRNN(20),\n",
    "keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcec2f",
   "metadata": {},
   "source": [
    "### using sequence to sequence model\n",
    "\n",
    "- The advantage of this is that the rnn will contain loss terms from all the time steps, not just output time step\n",
    "- This means many more error gradients flowing in the network\n",
    "- This both stabilizes and speed up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e472c714",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.empty((10000, n_steps, 10)) # each target is a sequence of 10Dvectors\n",
    "for step_ahead in range(1, 10 + 1):\n",
    "    Y[:, :, step_ahead - 1] = series[:, step_ahead:step_ahead + n_steps,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa3b149a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 50, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b6215d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(take in 3 dimensional matrixx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3ecd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = Y[:7000]\n",
    "Y_valid = Y[7000:9000]\n",
    "Y_test = Y[9000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a96b02",
   "metadata": {},
   "source": [
    "### The corresponding model will look like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None,\n",
    "    1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "\n",
    "# use time distributed to make a dense layer acccpet a sequence as inputs \n",
    "# same as Dense 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a229c",
   "metadata": {},
   "source": [
    "Since all the outputs are needed during training, but only the last time\n",
    "step is useful for prediction and for evaluation ,we will rely on MSE over all\n",
    "outputs for training, we will use a custom metric for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4aadee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_time_step_mse(y_true, y_pred):\n",
    "    return keras.metrics.mean_squared_error(y_true[:,-1], y_pred[:,-1])\n",
    "\n",
    "opitimizer = keras.optimizers.Adam(lr = 0.01)\n",
    "model.compile(loss = \"mse\", optimizer = optimizer, metrics = [last_time_step_mse])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8569d797",
   "metadata": {},
   "source": [
    "We get a validation MSE of about 0.006, which is 25% better than the\n",
    "previous model. You can combine this approach with the first one: just\n",
    "predict the next 10 values using this RNN, then concatenate these values to\n",
    "the input time series and use the model again to predict the next 10 values,\n",
    "and repeat the process as many times as needed. With this approach, you\n",
    "can generate arbitrarily long sequences. It may not be very accurate for\n",
    "long-term predictions, but it may be just fine if your goal is to generate\n",
    "original music or text, as we will see in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4092f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LNSimpleRNNCell(keras.layers.Layer):\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "    super().__init__(**kwargs)\n",
    "    self.state_size = units\n",
    "    self.output_size = units\n",
    "    self.simple_rnn_cell = keras.layers.SimpleRNNCell(units,\n",
    "    activation=None)\n",
    "    self.layer_norm = keras.layers.LayerNormalization()\n",
    "    self.activation = keras.activations.get(activation)\n",
    "    def call(self, inputs, states):\n",
    "    outputs, new_states = self.simple_rnn_cell(inputs, states)\n",
    "    norm_outputs = self.activation(self.layer_norm(outputs))\n",
    "    return norm_outputs, [norm_outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4d1eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True,\n",
    "    input_shape=[None, 1]),\n",
    "    keras.layers.RNN(LNSimpleRNNCell(20), return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f0e48e",
   "metadata": {},
   "source": [
    "### Dealing with short term memory (LSTM cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2a6350",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.LSTM(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.LSTM(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54e43d7",
   "metadata": {},
   "source": [
    "### Dealing with the fairly short term memort of the LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa3111",
   "metadata": {},
   "source": [
    "a 1D convolutional\n",
    "layer slides several kernels across a sequence, producing a 1D feature map\n",
    "per kernel. Each kernel will learn to detect a single very short sequential\n",
    "pattern (no longer than the kernel size). If you use 10 kernels, then the\n",
    "layerâ€™s output will be composed of 10 1-dimensional sequences (all of the\n",
    "same length), or equivalently you can view this output as a single 10-\n",
    "dimensional sequence. This means that you can build a neural network\n",
    "composed of a mix of recurrent layers and 1D convolutional layers (or\n",
    "even 1D pooling layers). If you use a 1D convolutional layer with a stride\n",
    "of 1 and \"same\" padding, then the output sequence will have the same\n",
    "length as the input sequence. But if you use \"valid\" padding or a stride\n",
    "greater than 1, then the output sequence will be shorter than the input\n",
    "sequence,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11e74c1",
   "metadata": {},
   "source": [
    "### Example model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a33d86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Conv1D(filters=20, kernel_size=4, strides=2,\n",
    "    padding=\"valid\",\n",
    "    input_shape=[None, 1]),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.GRU(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(10))\n",
    "])\n",
    "\n",
    "\n",
    "# the first layer keeps important information while also shortenniing the sequence making the lstm keep a longer memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb397dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train[:, 3::2], epochs=20,\n",
    "validation_data=(X_valid, Y_valid[:, 3::2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9be3b4",
   "metadata": {},
   "source": [
    "### WAVENET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b38d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.InputLayer(input_shape=[None, 1]))\n",
    "for rate in (1, 2, 4, 8) * 2:\n",
    "    model.add(keras.layers.Conv1D(filters=20, kernel_size=2,\n",
    "    padding=\"causal\",\n",
    "    activation=\"relu\", dilation_rate=rate))\n",
    "model.add(keras.layers.Conv1D(filters=10, kernel_size=1))\n",
    "\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[last_time_step_mse])\n",
    "history = model.fit(X_train, Y_train, epochs=20,\n",
    "validation_data=(X_valid, Y_valid))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
